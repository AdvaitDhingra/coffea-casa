distributed:
    version: 2
  
    dashboard:
      link: /user/{JUPYTERHUB_USER}/proxy/{port}/status
  
    scheduler:
      idle-timeout: 3600s
  
  # uncomment to force new worker pods after 2 hrs
  #  worker:
  #    lifetime:
  #      duration: "2 hours"
  #      stagger: "10 s"
  #      restart: true
  
    admin:
      tick:
        limit: 5s
  
    logging:
      distributed: warning
      bokeh: critical
      tornado: critical
      tornado.application: error

# Specific to each hub
#gateway:
  #  address: xxxx/services/dask-gateway/
  #  proxy-address: tls://xx.xxx.xx:8786
  
labextension:
  factory:
    module: 'dask_jobqueue'
    class: 'HTCondorCluster'
    args: []
    kwargs: {cores: 4,
              memory: "2GB",
              disk: "1GB",
              log_directory: "logs",
              silence_logs: "debug",
              scheduler_options: {"dashboard_address": 8786,"port": 8787, "external_address": "129.93.183.33:8787"},
              job_extra: {"universe": "docker",
                                     "encrypt_input_files": "/etc/cmsaf-secrets/xcache_token",
                                     "docker_image": "coffeateam/coffea-casa-analysis:latest", 
                                     "container_service_names": "dask",
                                     "dask_container_port": "8787",
                                     "should_transfer_files": "YES",
                                     "when_to_transfer_output": "ON_EXIT",
                                     "+DaskSchedulerAddress": '"129.93.183.33:8787"'}}
                                     

  initial:
    - name: "UNL HTCondor cluster"
      workers: 1
      adapt:
        minimum: 1
        maximum: 10
  